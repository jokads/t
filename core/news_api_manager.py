#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
NEWS API MANAGER ULTRA CORRIGIDO v4.0
Gerenciador de notÃ­cias do NewsAPI.org com correÃ§Ãµes completas
CORREÃ‡Ã•ES: Leitura correta do .env + Busca real de notÃ­cias + Cache inteligente
"""

import os
import sys
import json
import time
import logging
import threading
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
import sqlite3
from pathlib import Path
from core.my_types import TradeSignal, NewsArticle, TradeDirection, Signal
from core.my_types import TradeSignal
from requests.exceptions import HTTPError

# Imports com fallback
try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False
    print("âš ï¸ Requests nÃ£o disponÃ­vel - news manager funcionarÃ¡ em modo simulado")

try:
    from dotenv import load_dotenv
    # Carregar .env do diretÃ³rio atual
    env_path = Path('.env')
    if env_path.exists():
        load_dotenv(env_path)
        print(f"âœ… Arquivo .env carregado de: {env_path.absolute()}")
    else:
        print(f"âš ï¸ Arquivo .env nÃ£o encontrado em: {env_path.absolute()}")
    DOTENV_AVAILABLE = True
except ImportError:
    DOTENV_AVAILABLE = False
    print("âš ï¸ python-dotenv nÃ£o disponÃ­vel")

try:
    from textblob import TextBlob
    TEXTBLOB_AVAILABLE = True
except ImportError:
    TEXTBLOB_AVAILABLE = False
    print("âš ï¸ TextBlob nÃ£o disponÃ­vel â€” anÃ¡lise de sentimento desativada.")

# ConfiguraÃ§Ã£o de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('news_manager.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

class NewsAPIManager:
    """
    Gerenciador de notÃ­cias do NewsAPI.org
    Busca, processa e analisa notÃ­cias financeiras em tempo real
    """
    
    def __init__(self):
        self.version = "4.0 ULTRA CORRIGIDO"
        self.logger = logging.getLogger("news_api_manager")
    
        # ConfiguraÃ§Ã£o da API
        self.api_key = self._load_api_key()
        self.base_url = "https://newsapi.org/v2"
        self.endpoints = {
            'everything':    f"{self.base_url}/everything",
            'top_headlines': f"{self.base_url}/top-headlines",
            'sources':       f"{self.base_url}/sources"
        }
    
        # Cache de notÃ­cias
        self.cache          = {}
        self.rate_limited   = {}   # âœ… ESSA LINHA RESOLVE O PROBLEMA
        self.cache_duration = 900
    
        # Lista de sÃ­mbolos
        self.all_symbols   = ["USDJPY", "EURUSD", "BTCUSD"]
        self._batch_index  = 0
        self._batch_size   = 5
    
        # Thread de atualizaÃ§Ã£o automÃ¡tica
        self.auto_update_thread  = None
        self.auto_update_running = False
    
        # EstatÃ­sticas
        self.stats = {
            'total_fetched':   0,
            'total_processed': 0,
            'last_fetch_time': None,
            'api_calls_today': 0,
            'errors_count':    0
        }
    
        # Logging
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        )
        self.logger.info(f"ğŸ—ï¸ News API Manager v{self.version} inicializado")
        self.logger.info(f"ğŸ”‘ API Key: {'Configurada' if self.api_key else 'NÃ£o configurada'}")
    
        # Testa conexÃ£o com a API
        self._test_api_connection()


    
    def analyze_with_ai(self, text: str) -> dict:
        """
        Usa IA para gerar resumo, hipÃ³tese e risco de uma notÃ­cia.
        """
        if not text or not text.strip():
            return {"summary": "", "hypothesis": "", "risk": ""}

        prompt = (
            "Resuma e analise esta notÃ­cia:\n\n"
            f"{text}\n\n"
            "Responda em JSON:\n"
            "{"
            "\"summary\": \"...\","
            "\"hypothesis\": \"...\","
            "\"risk\": \"...\""
            "}"
        )

        # Aqui vocÃª chama sua IA (exemplo fictÃ­cio)
        response = self.ia_manager.ask_model(prompt)

        try:
            data = json.loads(response)
            return data
        except Exception:
            self.logger.warning("âš ï¸ Resposta IA invÃ¡lida, usando valores vazios.")
            return {"summary": "", "hypothesis": "", "risk": ""}




    def _load_api_key(self) -> Optional[str]:
        """
        Carregar API key do arquivo .env
        """
        try:
            api_key_vars = ['NEWS_API_KEY', 'NEWSAPI_KEY', 'NEWS_API_TOKEN']
            for var in api_key_vars:
                key = os.getenv(var)
                if key and key.strip() and key != 'YOUR_NEWS_API_KEY_HERE':
                    self.logger.info(f"âœ… API Key carregada da variÃ¡vel: {var}")
                    return key.strip()

            env_files = ['.env', '../.env', 'config/.env']
            for env_file in env_files:
                if os.path.exists(env_file):
                    with open(env_file, 'r', encoding='utf-8') as f:
                        for line in f:
                            for var in api_key_vars:
                                if line.startswith(f"{var}="):
                                    val = line.split('=',1)[1].strip().strip('"').strip("'")
                                    if val and val != 'YOUR_NEWS_API_KEY_HERE':
                                        self.logger.info(f"âœ… API Key encontrada no arquivo {env_file}")
                                        return val
            # Se nÃ£o achou em lugar nenhum:
            self.logger.warning("âš ï¸ API Key nÃ£o encontrada - modo simulado")
            return None

        except Exception as e:
            self.logger.error(f"âŒ Erro ao carregar API Key: {e}")
            return None
    


    def _fetch_next_batch(self):
        """
        Busca notÃ­cias para os prÃ³ximos self._batch_size sÃ­mbolos,
        avanÃ§ando self._batch_index (com wrap-around).
        """
        start = self._batch_index
        end   = start + self._batch_size
        batch = self.all_symbols[start:end]
        if end > len(self.all_symbols):
            batch += self.all_symbols[0:(end % len(self.all_symbols))]

        for symbol in batch:
            self.fetch_news_for(symbol)

        self._batch_index = (self._batch_index + self._batch_size) % len(self.all_symbols)


    def start_auto_update(self, interval_minutes: int = 30):
        """
        A cada interval_minutes, busca o prÃ³ximo batch de notÃ­cias.
        """
        if self.auto_update_running:
            self.logger.warning("âš ï¸ AtualizaÃ§Ã£o automÃ¡tica jÃ¡ estÃ¡ rodando")
            return

        self.auto_update_running = True

    
    
        # === define a funÃ§Ã£o de loop de atualizaÃ§Ã£o dentro do mÃ©todo ===
        def update_loop():
            while self.auto_update_running:
                self.logger.info(
                    f"ğŸ”„ Buscando prÃ³ximo batch de {self._batch_size} sÃ­mbolos"
                )
                self._fetch_next_batch()
                time.sleep(interval_minutes * 60)  # usa o interval_minutes do mÃ©todo

        # === cria e inicia a thread, ainda dentro do mÃ©todo ===
        self.auto_update_thread = threading.Thread(
            target=update_loop,
            daemon=True
        )
        self.auto_update_thread.start()

        self.logger.info(
            f"âœ… Auto-update iniciado (batch de {self._batch_size} a cada {interval_minutes}min)"
        )  


    def fetch_news_for(self, symbol: str) -> List[dict]:
        now = time.time()

        # 0) cooldown por rate-limit
        if symbol in self.rate_limited:
            if now - self.rate_limited[symbol] < 10 * 60:
                self.logger.warning(f"{symbol} em cooldown, retornando cache")
                return self.cache.get(symbol, {}).get('articles', [])
            else:
                del self.rate_limited[symbol]

        # 1) cache simples
        if symbol in self.cache and (now - self.cache[symbol]['timestamp']) < self.cache_duration:
           return self.cache[symbol]['articles']

        # 2) montar requisiÃ§Ã£o
        params = {
            'qInTitle': symbol,
            'language': 'pt',
            'sortBy': 'publishedAt',
            'pageSize': 20
        }
        headers = {'X-API-Key': self.api_key} if self.api_key else {}
        if not REQUESTS_AVAILABLE or not self.api_key:
           self.logger.warning("Modo simulado: sem requests ou API key")
           return []

        # 3) back-off com mÃ¡ximo de tentativas
        backoff, max_attempts = 1, 5
        for attempt in range(1, max_attempts + 1):
            try:
                resp = requests.get(
                    self.endpoints['everything'],
                    params=params,
                    headers=headers,
                    timeout=10
                )
                try:
                    resp.raise_for_status()
                except HTTPError:
                    if resp.status_code == 429:
                        if attempt < max_attempts:
                            self.logger.warning(f"[{attempt}/{max_attempts}] 429 para {symbol}, dormindo {backoff}s")
                            time.sleep(backoff)
                            backoff = min(backoff * 2, 60)
                            continue
                        self.logger.warning(f"429 persistente para {symbol}, marcando cooldown")
                        self.rate_limited[symbol] = time.time()
                        return self.cache.get(symbol, {}).get('articles', [])
                    else:
                        raise
                break  # sucesso
            except Exception as e:
                self.stats['errors_count'] += 1
                self.logger.error(f"âŒ Erro HTTP ao buscar notÃ­cias para {symbol} (tentativa {attempt}): {e}")
                if attempt == max_attempts:
                    return self.cache.get(symbol, {}).get('articles', [])

        # 4) processar resposta
        data = resp.json()
        articles = []

        for a in data.get('articles', []):
            content = a.get('content') or ""
            # Aqui chama a IA
            ai_analysis = self.analyze_with_ai(content)
            article_dict = {
                "title":        a.get('title'),
                "description":  a.get('description'),
                "content":      content,
                "url":          a.get('url'),
                "source":       a.get('source', {}).get('name'),
                "author":       a.get('author'),
                "published_at": a.get('publishedAt'),
                "ai_summary":   ai_analysis.get("summary"),
                "ai_hypothesis":ai_analysis.get("hypothesis"),
                "ai_risk":      ai_analysis.get("risk")
            }
            articles.append(article_dict)

        # 5) atualizar cache e retornar
        self.cache[symbol] = {'timestamp': now, 'articles': articles}
        return articles



    def _test_api_connection(self) -> bool:
        """
        Testar conexÃ£o com a API
        """
        if not self.api_key or not REQUESTS_AVAILABLE:
            self.logger.warning("âš ï¸ API nÃ£o disponÃ­vel - modo simulado ativado")
            return False
        
        try:
            headers = {'X-API-Key': self.api_key}
            response = requests.get(
                f"{self.base_url}/sources",
                headers=headers,
                timeout=10
            )
            
            if response.status_code == 200:
                self.logger.info("âœ… ConexÃ£o com NewsAPI estabelecida")
                return True
            elif response.status_code == 401:
                self.logger.error("âŒ API Key invÃ¡lida")
                return False
            elif response.status_code == 429:
                self.logger.warning("âš ï¸ Limite de requisiÃ§Ãµes excedido")
                return False
            else:
                self.logger.warning(f"âš ï¸ Resposta inesperada da API: {response.status_code}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ Erro ao testar conexÃ£o: {e}")
            return False
    
    def _init_database(self):
        """
        Inicializar banco de dados para cache de notÃ­cias
        """
        try:
            os.makedirs("data", exist_ok=True)
            
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Tabela de notÃ­cias
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS news (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    title TEXT NOT NULL,
                    description TEXT,
                    content TEXT,
                    url TEXT UNIQUE,
                    source TEXT,
                    author TEXT,
                    published_at TIMESTAMP,
                    category TEXT,
                    keywords TEXT,
                    sentiment_score REAL,
                    sentiment_label TEXT,
                    impact_score REAL,
                    relevance_score REAL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # Tabela de estatÃ­sticas
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS news_stats (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    date DATE DEFAULT CURRENT_DATE,
                    total_fetched INTEGER DEFAULT 0,
                    total_processed INTEGER DEFAULT 0,
                    api_calls INTEGER DEFAULT 0,
                    errors INTEGER DEFAULT 0,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # Ãndices para performance
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_news_published_at ON news(published_at)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_news_category ON news(category)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_news_sentiment ON news(sentiment_score)')
            
            conn.commit()
            conn.close()
            
            self.logger.info("âœ… Banco de dados inicializado")
            
        except Exception as e:
            self.logger.error(f"âŒ Erro ao inicializar banco: {e}")
    
    def fetch_forex_news(self, limit: int = 20) -> List[Dict[str, Any]]:
        """
        Buscar notÃ­cias relacionadas ao Forex
        """
        try:
            # Verificar cache
            cache_key = f"forex_news_{limit}"
            if self._is_cache_valid(cache_key):
                self.logger.info("ğŸ“‹ Retornando notÃ­cias do cache")
                return self.cache[cache_key]['data']
            
            if not self.api_key or not REQUESTS_AVAILABLE:
                return self._get_simulated_forex_news(limit)
            
            # Construir query de busca
            keywords = ' OR '.join(self.search_config['forex_keywords'][:10])  # Limitar keywords
            
            params = {
                'q': keywords,
                'language': 'en',
                'sortBy': 'publishedAt',
                'pageSize': min(limit, 100),  # MÃ¡ximo da API
                'domains': ','.join([
                    'reuters.com', 'bloomberg.com', 'cnbc.com',
                    'marketwatch.com', 'investing.com', 'forexfactory.com'
                ])
            }
            
            headers = {'X-API-Key': self.api_key}
            
            self.logger.info(f"ğŸ” Buscando notÃ­cias Forex: {keywords[:50]}...")
            
            response = requests.get(
                self.endpoints['everything'],
                headers=headers,
                params=params,
                timeout=15
            )
            
            self.stats['api_calls_today'] += 1
            
            if response.status_code == 200:
                data = response.json()
                articles = data.get('articles', [])
                
                # Processar notÃ­cias
                processed_news = []
                for article in articles:
                    processed_article = self._process_article(article, 'forex')
                    if processed_article:
                        processed_news.append(processed_article)
                
                # Salvar no cache
                self.cache[cache_key] = {
                    'data': processed_news,
                    'timestamp': datetime.now()
                }
                
                # Salvar no banco
                self._save_news_to_db(processed_news)
                
                self.stats['total_fetched'] += len(articles)
                self.stats['total_processed'] += len(processed_news)
                self.stats['last_fetch_time'] = datetime.now()
                
                self.logger.info(f"âœ… {len(processed_news)} notÃ­cias Forex processadas")
                return processed_news
                
            elif response.status_code == 429:
                self.logger.warning("âš ï¸ Limite de API excedido - usando cache/simulaÃ§Ã£o")
                return self._get_cached_or_simulated_news('forex', limit)
            else:
                self.logger.error(f"âŒ Erro na API: {response.status_code}")
                return self._get_cached_or_simulated_news('forex', limit)
                
        except Exception as e:
            self.logger.error(f"âŒ Erro ao buscar notÃ­cias Forex: {e}")
            self.stats['errors_count'] += 1
            return self._get_cached_or_simulated_news('forex', limit)
    
    def fetch_market_news(self, limit: int = 15) -> List[Dict[str, Any]]:
        """
        Buscar notÃ­cias do mercado financeiro
        """
        try:
            cache_key = f"market_news_{limit}"
            if self._is_cache_valid(cache_key):
                return self.cache[cache_key]['data']
            
            if not self.api_key or not REQUESTS_AVAILABLE:
                return self._get_simulated_market_news(limit)
            
            keywords = ' OR '.join(self.search_config['market_keywords'][:8])
            
            params = {
                'q': keywords,
                'language': 'en',
                'sortBy': 'publishedAt',
                'pageSize': min(limit, 100),
                'category': 'business'
            }
            
            headers = {'X-API-Key': self.api_key}
            
            response = requests.get(
                self.endpoints['everything'],
                headers=headers,
                params=params,
                timeout=15
            )
            
            self.stats['api_calls_today'] += 1
            
            if response.status_code == 200:
                data = response.json()
                articles = data.get('articles', [])
                
                processed_news = []
                for article in articles:
                    processed_article = self._process_article(article, 'market')
                    if processed_article:
                        processed_news.append(processed_article)
                
                self.cache[cache_key] = {
                    'data': processed_news,
                    'timestamp': datetime.now()
                }
                
                self._save_news_to_db(processed_news)
                
                self.logger.info(f"âœ… {len(processed_news)} notÃ­cias de mercado processadas")
                return processed_news
            else:
                return self._get_cached_or_simulated_news('market', limit)
                
        except Exception as e:
            self.logger.error(f"âŒ Erro ao buscar notÃ­cias de mercado: {e}")
            return self._get_cached_or_simulated_news('market', limit)
    
    def fetch_economic_news(self, limit: int = 10) -> List[Dict[str, Any]]:
        """
        Buscar notÃ­cias econÃ´micas
        """
        try:
            cache_key = f"economic_news_{limit}"
            if self._is_cache_valid(cache_key):
                return self.cache[cache_key]['data']
            
            if not self.api_key or not REQUESTS_AVAILABLE:
                return self._get_simulated_economic_news(limit)
            
            keywords = ' OR '.join(self.search_config['economic_keywords'])
            
            params = {
                'q': keywords,
                'language': 'en',
                'sortBy': 'publishedAt',
                'pageSize': min(limit, 50),
                'domains': 'reuters.com,bloomberg.com,cnbc.com'
            }
            
            headers = {'X-API-Key': self.api_key}
            
            response = requests.get(
                self.endpoints['everything'],
                headers=headers,
                params=params,
                timeout=15
            )
            
            self.stats['api_calls_today'] += 1
            
            if response.status_code == 200:
                data = response.json()
                articles = data.get('articles', [])
                
                processed_news = []
                for article in articles:
                    processed_article = self._process_article(article, 'economic')
                    if processed_article:
                        processed_news.append(processed_article)
                
                self.cache[cache_key] = {
                    'data': processed_news,
                    'timestamp': datetime.now()
                }
                
                self._save_news_to_db(processed_news)
                
                self.logger.info(f"âœ… {len(processed_news)} notÃ­cias econÃ´micas processadas")
                return processed_news
            else:
                return self._get_cached_or_simulated_news('economic', limit)
                
        except Exception as e:
            self.logger.error(f"âŒ Erro ao buscar notÃ­cias econÃ´micas: {e}")
            return self._get_cached_or_simulated_news('economic', limit)
    
    def get_all_news(self, limit: int = 50) -> Dict[str, List[Dict[str, Any]]]:
        """
        Buscar todas as categorias de notÃ­cias
        """
        try:
            self.logger.info("ğŸ“° Buscando todas as categorias de notÃ­cias...")
            
            all_news = {
                'forex': self.fetch_forex_news(limit // 3),
                'market': self.fetch_market_news(limit // 3),
                'economic': self.fetch_economic_news(limit // 3)
            }
            
            # Combinar e ordenar por data
            combined_news = []
            for category, news_list in all_news.items():
                combined_news.extend(news_list)
            
            # Ordenar por data de publicaÃ§Ã£o
            combined_news.sort(key=lambda x: x.get('published_at', ''), reverse=True)
            
            # Limitar ao nÃºmero solicitado
            combined_news = combined_news[:limit]
            
            all_news['combined'] = combined_news
            
            self.logger.info(f"âœ… Total de {len(combined_news)} notÃ­cias obtidas")
            return all_news
            
        except Exception as e:
            self.logger.error(f"âŒ Erro ao buscar todas as notÃ­cias: {e}")
            return {
                'forex': self._get_simulated_forex_news(10),
                'market': self._get_simulated_market_news(10),
                'economic': self._get_simulated_economic_news(10),
                'combined': []
            }
    
    def _process_article(self, article: Dict[str, Any], category: str) -> Optional[Dict[str, Any]]:
        """
        Processar artigo individual
        """
        try:
            if not article.get('title') or not article.get('url'):
                return None
            
            # AnÃ¡lise de sentimento
            sentiment = self._analyze_sentiment(article.get('title', '') + ' ' + article.get('description', ''))
            
            # Calcular relevÃ¢ncia
            relevance = self._calculate_relevance(article, category)
            
            # Calcular impacto
            impact = self._calculate_impact(article, sentiment)
            
            processed = {
                'title': article.get('title', '').strip(),
                'description': article.get('description', '').strip(),
                'content': article.get('content', '').strip(),
                'url': article.get('url', ''),
                'source': article.get('source', {}).get('name', 'Unknown'),
                'author': article.get('author', ''),
                'published_at': article.get('publishedAt', ''),
                'category': category,
                'keywords': self._extract_keywords(article, category),
                'sentiment_score': sentiment['score'],
                'sentiment_label': sentiment['label'],
                'impact_score': impact,
                'relevance_score': relevance,
                'processed_at': datetime.now().isoformat()
            }
            
            return processed
            
        except Exception as e:
            self.logger.error(f"âŒ Erro ao processar artigo: {e}")
            return None
    
    def _analyze_sentiment(self, text: str) -> Dict[str, Any]:
        """
        Analisar sentimento do texto
        """
        try:
            if TEXTBLOB_AVAILABLE and text:
                blob = TextBlob(text)
                polarity = blob.sentiment.polarity
                
                if polarity > 0.1:
                    label = 'positive'
                elif polarity < -0.1:
                    label = 'negative'
                else:
                    label = 'neutral'
                
                return {
                    'score': polarity,
                    'label': label
                }
            else:
                # AnÃ¡lise simples baseada em palavras-chave
                positive_words = ['gain', 'rise', 'up', 'bull', 'growth', 'strong', 'positive']
                negative_words = ['fall', 'drop', 'down', 'bear', 'decline', 'weak', 'negative']
                
                text_lower = text.lower()
                positive_count = sum(1 for word in positive_words if word in text_lower)
                negative_count = sum(1 for word in negative_words if word in text_lower)
                
                if positive_count > negative_count:
                    return {'score': 0.3, 'label': 'positive'}
                elif negative_count > positive_count:
                    return {'score': -0.3, 'label': 'negative'}
                else:
                    return {'score': 0.0, 'label': 'neutral'}
                    
        except Exception as e:
            self.logger.error(f"âŒ Erro na anÃ¡lise de sentimento: {e}")
            return {'score': 0.0, 'label': 'neutral'}
    
    def _calculate_relevance(self, article: Dict[str, Any], category: str) -> float:
        """
        Calcular relevÃ¢ncia do artigo
        """
        try:
            relevance_score = 0.0
            text = (article.get('title', '') + ' ' + article.get('description', '')).lower()
            
            # PontuaÃ§Ã£o baseada em palavras-chave da categoria
            keywords = self.search_config.get(f'{category}_keywords', [])
            for keyword in keywords:
                if keyword.lower() in text:
                    relevance_score += 0.1
            
            # PontuaÃ§Ã£o baseada na fonte
            source = article.get('source', {}).get('name', '').lower()
            if any(trusted in source for trusted in self.trusted_sources):
                relevance_score += 0.3
            
            # PontuaÃ§Ã£o baseada na recÃªncia
            published_at = article.get('publishedAt', '')
            if published_at:
                try:
                    pub_date = datetime.fromisoformat(published_at.replace('Z', '+00:00'))
                    hours_ago = (datetime.now() - pub_date.replace(tzinfo=None)).total_seconds() / 3600
                    if hours_ago < 24:
                        relevance_score += 0.2
                    elif hours_ago < 48:
                        relevance_score += 0.1
                except:
                    pass
            
            return min(relevance_score, 1.0)
            
        except Exception as e:
            self.logger.error(f"âŒ Erro ao calcular relevÃ¢ncia: {e}")
            return 0.5
    
    def _calculate_impact(self, article: Dict[str, Any], sentiment: Dict[str, Any]) -> float:
        """
        Calcular impacto potencial do artigo
        """
        try:
            impact_score = 0.5  # Base
            
            # Impacto baseado no sentimento
            sentiment_score = abs(sentiment.get('score', 0))
            impact_score += sentiment_score * 0.3
            
            # Impacto baseado em palavras-chave de alto impacto
            high_impact_words = [
                'federal reserve', 'ecb', 'interest rate', 'inflation',
                'recession', 'crisis', 'crash', 'emergency', 'breaking'
            ]
            
            text = (article.get('title', '') + ' ' + article.get('description', '')).lower()
            for word in high_impact_words:
                if word in text:
                    impact_score += 0.2
            
            return min(impact_score, 1.0)
            
        except Exception as e:
            self.logger.error(f"âŒ Erro ao calcular impacto: {e}")
            return 0.5
    
    def _extract_keywords(self, article: Dict[str, Any], category: str) -> str:
        """
        Extrair palavras-chave do artigo
        """
        try:
            text = (article.get('title', '') + ' ' + article.get('description', '')).lower()
            keywords = []
            
            # Buscar palavras-chave da categoria
            category_keywords = self.search_config.get(f'{category}_keywords', [])
            for keyword in category_keywords:
                if keyword.lower() in text:
                    keywords.append(keyword)
            
            return ', '.join(keywords[:5])  # MÃ¡ximo 5 keywords
            
        except Exception as e:
            self.logger.error(f"âŒ Erro ao extrair keywords: {e}")
            return ''
    
    def _save_news_to_db(self, news_list: List[Dict[str, Any]]):
        """
        Salvar notÃ­cias no banco de dados
        """
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            for news in news_list:
                cursor.execute('''
                    INSERT OR REPLACE INTO news (
                        title, description, content, url, source, author,
                        published_at, category, keywords, sentiment_score,
                        sentiment_label, impact_score, relevance_score
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    news.get('title'),
                    news.get('description'),
                    news.get('content'),
                    news.get('url'),
                    news.get('source'),
                    news.get('author'),
                    news.get('published_at'),
                    news.get('category'),
                    news.get('keywords'),
                    news.get('sentiment_score'),
                    news.get('sentiment_label'),
                    news.get('impact_score'),
                    news.get('relevance_score')
                ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"âŒ Erro ao salvar no banco: {e}")
    
    def _is_cache_valid(self, cache_key: str) -> bool:
        """
        Verificar se o cache Ã© vÃ¡lido
        """
        if cache_key not in self.cache:
            return False
        
        cache_time = self.cache[cache_key]['timestamp']
        return (datetime.now() - cache_time).seconds < self.cache_duration
    
    def _get_cached_or_simulated_news(self, category: str, limit: int) -> List[Dict[str, Any]]:
        """
        Obter notÃ­cias do cache ou simuladas
        """
        try:
            # Tentar cache primeiro
            cache_key = f"{category}_news_{limit}"
            if cache_key in self.cache:
                return self.cache[cache_key]['data']
            
            # Tentar banco de dados
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT title, description, content, url, source, author,
                       published_at, category, keywords, sentiment_score,
                       sentiment_label, impact_score, relevance_score
                FROM news 
                WHERE category = ? 
                ORDER BY published_at DESC 
                LIMIT ?
            ''', (category, limit))
            
            rows = cursor.fetchall()
            conn.close()
            
            if rows:
                news_list = []
                for row in rows:
                    news_list.append({
                        'title': row[0],
                        'description': row[1],
                        'content': row[2],
                        'url': row[3],
                        'source': row[4],
                        'author': row[5],
                        'published_at': row[6],
                        'category': row[7],
                        'keywords': row[8],
                        'sentiment_score': row[9],
                        'sentiment_label': row[10],
                        'impact_score': row[11],
                        'relevance_score': row[12]
                    })
                return news_list
            
            # Fallback para notÃ­cias simuladas
            if category == 'forex':
                return self._get_simulated_forex_news(limit)
            elif category == 'market':
                return self._get_simulated_market_news(limit)
            elif category == 'economic':
                return self._get_simulated_economic_news(limit)
            else:
                return []
                
        except Exception as e:
            self.logger.error(f"âŒ Erro ao obter notÃ­cias: {e}")
            return []
    
    def _get_simulated_forex_news(self, limit: int) -> List[Dict[str, Any]]:
        """
        Gerar notÃ­cias Forex simuladas
        """
        simulated_news = [
            {
                'title': 'EUR/USD mantÃ©m-se estÃ¡vel apÃ³s dados do PIB da Eurozona',
                'description': 'O par EUR/USD negocia em torno de 1.1000 apÃ³s a divulgaÃ§Ã£o dos dados do PIB da Eurozona que ficaram em linha com as expectativas.',
                'content': 'AnÃ¡lise detalhada dos movimentos do EUR/USD...',
                'url': 'https://example.com/news/1',
                'source': 'Reuters',
                'author': 'Analista Forex',
                'published_at': datetime.now().isoformat(),
                'category': 'forex',
                'keywords': 'EUR/USD, PIB, Eurozona',
                'sentiment_score': 0.1,
                'sentiment_label': 'neutral',
                'impact_score': 0.6,
                'relevance_score': 0.8
            },
            {
                'title': 'Federal Reserve sinaliza possÃ­vel pausa nas subidas de juros',
                'description': 'Membros do Fed indicam que podem pausar o ciclo de aperto monetÃ¡rio se a inflaÃ§Ã£o continuar a desacelerar.',
                'content': 'AnÃ¡lise das declaraÃ§Ãµes do Federal Reserve...',
                'url': 'https://example.com/news/2',
                'source': 'Bloomberg',
                'author': 'Correspondente EconÃ´mico',
                'published_at': (datetime.now() - timedelta(hours=2)).isoformat(),
                'category': 'forex',
                'keywords': 'Federal Reserve, juros, inflaÃ§Ã£o',
                'sentiment_score': 0.3,
                'sentiment_label': 'positive',
                'impact_score': 0.9,
                'relevance_score': 0.9
            },
            {
                'title': 'GBP/USD sob pressÃ£o apÃ³s dados de emprego do Reino Unido',
                'description': 'A libra enfraquece contra o dÃ³lar apÃ³s dados de emprego decepcionantes do Reino Unido.',
                'content': 'AnÃ¡lise dos dados de emprego britÃ¢nicos...',
                'url': 'https://example.com/news/3',
                'source': 'Financial Times',
                'author': 'Analista de Mercados',
                'published_at': (datetime.now() - timedelta(hours=4)).isoformat(),
                'category': 'forex',
                'keywords': 'GBP/USD, emprego, Reino Unido',
                'sentiment_score': -0.4,
                'sentiment_label': 'negative',
                'impact_score': 0.7,
                'relevance_score': 0.8
            }
        ]
        
        return simulated_news[:limit]
    
    def _get_simulated_market_news(self, limit: int) -> List[Dict[str, Any]]:
        """
        Gerar notÃ­cias de mercado simuladas
        """
        simulated_news = [
            {
                'title': 'S&P 500 fecha em alta com otimismo sobre resultados corporativos',
                'description': 'O Ã­ndice S&P 500 subiu 0.8% com investidores otimistas sobre a temporada de resultados.',
                'content': 'AnÃ¡lise do fechamento dos mercados americanos...',
                'url': 'https://example.com/market/1',
                'source': 'CNBC',
                'author': 'RepÃ³rter de Mercados',
                'published_at': datetime.now().isoformat(),
                'category': 'market',
                'keywords': 'S&P 500, resultados, otimismo',
                'sentiment_score': 0.5,
                'sentiment_label': 'positive',
                'impact_score': 0.7,
                'relevance_score': 0.8
            },
            {
                'title': 'Volatilidade aumenta nos mercados asiÃ¡ticos',
                'description': 'Mercados asiÃ¡ticos mostram maior volatilidade em meio a incertezas geopolÃ­ticas.',
                'content': 'AnÃ¡lise da sessÃ£o asiÃ¡tica...',
                'url': 'https://example.com/market/2',
                'source': 'MarketWatch',
                'author': 'Correspondente AsiÃ¡tico',
                'published_at': (datetime.now() - timedelta(hours=6)).isoformat(),
                'category': 'market',
                'keywords': 'volatilidade, Ãsia, geopolÃ­tica',
                'sentiment_score': -0.2,
                'sentiment_label': 'negative',
                'impact_score': 0.6,
                'relevance_score': 0.7
            }
        ]
        
        return simulated_news[:limit]
    
    def _get_simulated_economic_news(self, limit: int) -> List[Dict[str, Any]]:
        """
        Gerar notÃ­cias econÃ´micas simuladas
        """
        simulated_news = [
            {
                'title': 'InflaÃ§Ã£o da Eurozona desacelera para 2.1% em novembro',
                'description': 'A inflaÃ§Ã£o anual da Eurozona caiu para 2.1%, aproximando-se da meta do BCE de 2%.',
                'content': 'AnÃ¡lise dos dados de inflaÃ§Ã£o europeus...',
                'url': 'https://example.com/economic/1',
                'source': 'Reuters',
                'author': 'Correspondente EconÃ´mico',
                'published_at': datetime.now().isoformat(),
                'category': 'economic',
                'keywords': 'inflaÃ§Ã£o, Eurozona, BCE',
                'sentiment_score': 0.3,
                'sentiment_label': 'positive',
                'impact_score': 0.8,
                'relevance_score': 0.9
            }
        ]
        
        return simulated_news[:limit]
    
    def start_auto_update(self, interval_minutes: int = 15):
        """
        Iniciar atualizaÃ§Ã£o automÃ¡tica de notÃ­cias
        """
        if self.auto_update_running:
            self.logger.warning("âš ï¸ AtualizaÃ§Ã£o automÃ¡tica jÃ¡ estÃ¡ rodando")
            return
        
        self.auto_update_running = True
        
        def update_loop():
            while self.auto_update_running:
                try:
                    self.logger.info("ğŸ”„ AtualizaÃ§Ã£o automÃ¡tica de notÃ­cias...")
                    self.get_all_news()
                    time.sleep(interval_minutes * 60)
                except Exception as e:
                    self.logger.error(f"âŒ Erro na atualizaÃ§Ã£o automÃ¡tica: {e}")
                    time.sleep(60)  # Esperar 1 minuto antes de tentar novamente
        
        self.auto_update_thread = threading.Thread(target=update_loop, daemon=True)
        self.auto_update_thread.start()
        
        self.logger.info(f"âœ… AtualizaÃ§Ã£o automÃ¡tica iniciada (intervalo: {interval_minutes} min)")
    
    def stop_auto_update(self):
        """
        Parar atualizaÃ§Ã£o automÃ¡tica
        """
        self.auto_update_running = False
        if self.auto_update_thread:
            self.auto_update_thread.join(timeout=5)
        self.logger.info("ğŸ›‘ AtualizaÃ§Ã£o automÃ¡tica parada")
    
    def get_stats(self) -> Dict[str, Any]:
        """
        Obter estatÃ­sticas do gerenciador
        """
        return {
            'version': self.version,
            'api_key_configured': bool(self.api_key),
            'api_available': bool(self.api_key and REQUESTS_AVAILABLE),
            'cache_size': len(self.cache),
            'auto_update_running': self.auto_update_running,
            'stats': self.stats.copy()
        }
    
    def clear_cache(self):
        """
        Limpar cache de notÃ­cias
        """
        self.cache.clear()
        self.logger.info("ğŸ—‘ï¸ Cache de notÃ­cias limpo")
    
    def __del__(self):
        """
        Destrutor - parar threads
        """
        try:
            self.stop_auto_update()
        except:
            pass

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FUNÃ‡Ã•ES DE CONVENIÃŠNCIA
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def create_news_manager() -> NewsAPIManager:
    """
    Criar instÃ¢ncia do gerenciador de notÃ­cias
    """
    return NewsAPIManager()

def get_latest_forex_news(limit: int = 10) -> List[Dict[str, Any]]:
    """
    Obter Ãºltimas notÃ­cias Forex
    """
    manager = create_news_manager()
    return manager.fetch_forex_news(limit)

def get_latest_market_news(limit: int = 10) -> List[Dict[str, Any]]:
    """
    Obter Ãºltimas notÃ­cias de mercado
    """
    manager = create_news_manager()
    return manager.fetch_market_news(limit)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TESTE E DEMONSTRAÃ‡ÃƒO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    try:
        print("ğŸ—ï¸ Testando News API Manager v4.0 ULTRA CORRIGIDO")
        print("=" * 60)
        
        # Criar manager
        manager = NewsAPIManager()
        
        # Mostrar estatÃ­sticas
        stats = manager.get_stats()
        print(f"ğŸ“Š EstatÃ­sticas:")
        print(f"   API Key: {'âœ… Configurada' if stats['api_key_configured'] else 'âŒ NÃ£o configurada'}")
        print(f"   API DisponÃ­vel: {'âœ… Sim' if stats['api_available'] else 'âŒ NÃ£o'}")
        print(f"   Cache: {stats['cache_size']} itens")
        print()
        
        # Buscar notÃ­cias
        print("ğŸ” Buscando notÃ­cias Forex...")
        forex_news = manager.fetch_forex_news(5)
        
        print(f"âœ… {len(forex_news)} notÃ­cias Forex encontradas:")
        for i, news in enumerate(forex_news[:3], 1):
            print(f"   {i}. {news['title'][:60]}...")
            print(f"      Fonte: {news['source']} | Sentimento: {news['sentiment_label']}")
        print()
        
        # Buscar todas as notÃ­cias
        print("ğŸ” Buscando todas as categorias...")
        all_news = manager.get_all_news(10)
        
        print(f"âœ… Total de notÃ­cias por categoria:")
        for category, news_list in all_news.items():
            if category != 'combined':
                print(f"   {category.capitalize()}: {len(news_list)} notÃ­cias")
        print()
        
        # Mostrar estatÃ­sticas finais
        final_stats = manager.get_stats()
        print(f"ğŸ“ˆ EstatÃ­sticas finais:")
        print(f"   Total buscado: {final_stats['stats']['total_fetched']}")
        print(f"   Total processado: {final_stats['stats']['total_processed']}")
        print(f"   Chamadas API hoje: {final_stats['stats']['api_calls_today']}")
        print(f"   Erros: {final_stats['stats']['errors_count']}")
        
        print("\nâœ… Teste concluÃ­do com sucesso!")
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ Teste interrompido pelo usuÃ¡rio")
    except Exception as e:
        print(f"\nâŒ Erro no teste: {e}")
        import traceback
        traceback.print_exc()

